* Goals

- Pretty efficient to interpret directly.
  - no SSA.
- Register machine, not stack machine
  - some research suggests that reg machines are more performant
  - but mostly because I feel like it.
- Support precise gc.
- Support closures.
  - Non-naive view of environment capture.
    (don't prevent a var from being gced due to a closure that never uses it)

* General notes

Rips off Lua pretty heavily.

Register machine. Each function indicates how many registers it needs.

Arguments passed on stack? In regs?

Three op form or two op as necessary for space constraints. Figure out later.

* Upvals & closures

Each fn has environment consisting of "upvals" (closed-over variables; name
stolen from lua, though ours are slightly different).

Closures are created by a "closure" instruction, which takes destination
register, function, and list of operands to populate environment upvals with.
Operands are:
- our registers
- our upvals

Upvals are not indirected; an environment directly contains the upvals needed.
This means they are *copied* when closed-over, so mutations to the register or
upval they came from will not propagate to them and vice versa.

** Big picture re upval copying semantics

There are three possible source-language semantics for variables & closures:
- immutable variables
- mutable variables with copying semantics
- mutable variables with sharing semantics

RVM makes the first two easy to implement efficiently, but an implementation of
the last needs to do some simple analyses to generate efficient RVM code. To
allow sharing, it needs to allocate heap space (perhaps in the form of ref
cells) for the shared variables. But putting every variable on the heap is bad.
So the compiler should only put variables on the heap if they are both (a)
shared between a parent and any of its transitive child functions and (b)
mutated by one of these functions. This is a pretty simple analysis to do, and a
relatively uncommon case in practice.

* Constants

Small numeric constants can be "immediate" (special operand form)?
Medium numeric constants can be loaded by an instruction.
Large numbers and other constants (eg. strings) are stored along with upvals
  and accessed the same way.

* Calling and return convention

Assume metadata-based precise gc.

Lua explicitly copies return values into place. This makes it possible for a
function to return things not in reg 0..n without explicitly moving its results
into place, probably a good thing. Might be in want of a fast path, though. (In
how many cases can we manage to get return values in registers 0..n w/o
copying?)

Lua also does tailcalls by setting up a frame as usual and then moving the frame
down. Again, allows tailcalling something without overwriting your own args /
explicitly moving args into place. Might turn out to be possible to avoid having
to do this via clever compilation, though. (Could we just fast-path tailcalls
whose args start at 0?)

Maybe just expose a "copy register range" instruction? Cleaner perhaps, but
probably slower. Might be useful anyways, but only add if actually needed.

- mmove a b n
  copies b..b+n to a..a+n. expects a < b.

* Intra-function labels, function labels, jumps and calls

Intra-function jumps are relative (pc offsets).

* Precise GC support

Tag bits for now.
_Try_ to make the C API generic enough to work with any.

Options:
1. tag bits. sml does this.

   pros:
   + pretty efficient, space- & time-wise

   cons:
   + not strictly portable (but damn close).
   + fucking the semantics of your language for its implementation.
     ie. "ugh 31 bit ints."

2. gc metadata: structure tags(&maps?), register&stack maps. makes function call
   interface "interesting".

   pros: the best.

   cons:
   + hardest to implement.
   + nigh-impossible to use from a dynamically-typed source language.
     consider (\x. f x)

3. large values (ugh large values). lua does this.

   pros:
   + can make doubles immediate too.

   cons:
   + memory inflation.
   + slow?

4. no immediate numbers. python does this.

   pros:
   + balls simple.

   cons:
   + slooooow (esp if you don't cache small ints)


* Instruction encoding notes

** Comparisons

This section is irrelevant for now, since we're not actually including an
integer comparison instruction yet.

Encoding comparisons is an interesting design point.

We take two operands, and each one could be register, upval, or immediate,
_except_ that we can rule out immediate/immediate comparison. This makes

    8 = 3*3 - 1

possibilities. However, encoding this in the minimum possible 3 bits is a PITA;
the natural encoding uses 4, with 2 bits each to specify the type (reg, upval,
imm) of each operand.

We can make do with only two comparison operations (eg. LEQ, EQ) if we're
willing to be constrained as to which branch goes where. Otherwise we want four
(LT, GEQ, EQ, NEQ). Taking the conditional is cheaper than not taking it, since
we just skip over next instruction without reading it. So not constraining
enables better optimization/performance-tweaking.

The best-performance option is probably an opcode for each combination of
comparison operation and operand types. At minimum there are 8 * 2 = 16
combinations, and at maximum there are 9 * 4 = 36. Writing the code for each
case manually would be insane, but some code-generation scheme could probably be
worked out.

For now, however, we take the simplest option: there is *one* comparison
instruction. It takes the two operands, along with a byte indicating (a) what
types the operands have (reg, upval, or imm) and (b) which comparison is desired
and. (a) is encoded in 4 bits (with the immediate/immediate case representable
but outlawed; this prohibition may or may not be enforced by the bytecode
interpreter) and (b) in 2 bits, so the whole thing can fit in a byte.

If we want our comparison ops to also support floating-point operands with IEEE
semantics, the story gets even more complicated. I'm not worrying about that for
now.

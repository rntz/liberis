* Goals

- Pretty efficient to interpret directly.
  - no SSA.
- Register machine, not stack machine
  - some research suggests that reg machines are more performant
  - but mostly because I feel like it.
- Support precise gc.
- Support closures.
  - Non-naive view of environment capture.
    (don't prevent a var from being gced due to a closure that never uses it)

* Tags

Current scheme: (WORDSIZE-1)-bit ints have low bit high. Everything else is a
pointer to an allocated block with a header tag. This is good if I'm doing
modulo arithmetic on numbers, because modulo-31 is bearable. But if I'm doing a
full numeric stack, then other options come to mind.

In particular, use more than 1 tag bit. I can use at most 3 tag bits on a
pointer while still being portable to 32-bit systems, which is desirable. With a
variable-length encoding I could use more tag bits, but the only non-pointer
values I'm likely to want are small integers (and maybe nil).

The immediate problems are:
1. 3 bits = 8 datatypes, which is probably not enough.
2. Host program might want to define its own datatypes.

Both can be solved in the same fashion: Really important datatypes get a 3-bit
tag. Otherwise, there is a 3-bit tag for "block with header tag", and the header
tag is a pointer to type metadata as in current scheme.

Benefits to this scheme:
1. Lower memory overhead (eg. conses go from 3-word to 2-word).
2. Possible GC speedup, since GC can switch on 3-bit tag and special-case the
   common datatypes. However, could do this with pointer-based scheme as well
   (though as if/else chain, not switch).

Disbenefits to it:
1. Need full numeric stack. 29-bit 2's complement arithmetic is not bearable.
   However, probably wanted this anyway.
2. Slight extra overhead on dealing with "uncommon" types.
3. It makes the "undefined" state of cells more interesting, and possibly less
   efficient, to encode.

Conclusion: Probably a good idea.

# What tags would we use?
1. Fixnum
2. Closure
3. Builtin
4. Cons
5. ? Symbol
6. ? String
7. ? Boxed number
8. Tagged object (? or nil if null)

Unfortunately this is a decision that would optimally be made with the aid of
hard evidence on the performance impacts, but I have no such evidence.

Candidates:
- *Nil/empty list*

  If nil is just a symbol, this is infeasible. On the other hand, could make
  symbols 3-bit tagged. Then comparing against nil is slightly more interesting.
  Could solve "compare with nil" by pre-interning nil, and having it be a
  statically known value. This seems perhaps the best option, as long as I'm
  okay with nil being a symbol.

  Alternatively, nil could have the 3-bit tag of a type that doesn't contain it,
  but whose content is a pointer (not an immediate), and be a NULL pointer. This
  puts extra overhead on the usage of that type, though. Likely types:
  - cons (nil is "empty list"; punnery on use of conses for lists)
  - symbol (is nil a symbol?)
  - tagged object (if we're gonna have extra overhead, put it on the uncommon case)

- *String*
  String manipulation is fairly common.

- *Symbol*
  How common is symbol manipulation, anyway? We don't do it when looking up
  globals. OTOH, if we use symbols for representing branches of ADTs etc, then
  checking symbol equality will be common.

- *Extnum/Boxed number*
  Not sure this is a good idea. Boxed numbers will need tags indicating their
  representation (large integer, rational, float) anyway, so why not just tagged
  object?

Non-candidates:
- *Cell*

  Loading through cells is so common that LOAD_CELL doesn't check that the thing
  it's loading through is in fact a cell; ensuring that is up to the compiler.
  So this doesn't actually need to be 3-bit tagged!

* Builtins

The original plan was to implement builtins (eg. cons, car, cdr, +, -, *, /) as
C functions. Perhaps as "special" C functions that don't get the normal
stack-based treatment, or perhaps not. But in either case, significant overhead
for calling them. Instead we could have another type, builtins, that the
compiler knows how to handle. Code involving "cons" doesn't get compiled
specially - it's still a call through a cell, so "cons" can be
overridden/redefined. But when we actually run the call instruction, it simply
notes that the "function" value is a builtin, and switches on it.

* General notes

Rips off Lua pretty heavily. Register machine, each function indicates how many
registers it needs, registers implicitly on a stack, calling function shifts
register window

* Upvals & closures

Each fn has environment consisting of "upvals" (closed-over variables; name
stolen from lua, though ours are slightly different).

Closures are created by a "closure" instruction, which takes destination
register, function, and list of operands to populate environment upvals with.
Operands are:
- our registers
- our upvals

Upvals are not indirected; a closure directly contains the upvals needed. This
means they are *copied* when closed-over, so mutations to the register or upval
they came from will not propagate to them and vice versa. This is in contrast to
lua's upvals.

** Big picture re upval copying semantics

There are three possible source-language semantics for variables & closures:
- immutable variables
- mutable variables with copying semantics
- mutable variables with sharing semantics

RVM makes the first two easy to implement efficiently, but an implementation of
the last needs to do some simple analyses to generate efficient RVM code. To
allow sharing, it needs to allocate heap space (perhaps in the form of ref
cells) for the shared variables. But putting every variable on the heap is bad.
So the compiler should only put variables on the heap if they are both (a)
shared between a parent and any of its transitive child functions and (b)
mutated by one of these functions. This is a pretty simple analysis to do, and a
relatively uncommon case in practice.

In fact, there is a name for this analysis/optimization: it is called
"assignment conversion", and (unsurprisingly) appears to have originated in the
design of optimizing scheme compilers. See "ORBIT: An optimizing compiler for
scheme", David Andrew Krantz, 1988.

* Constants

General mechanism for constants is to put them into the upvals of a closure.
Since /all/ functions are closures (no special-case for toplevel funcs), this
always works. I may decide later to add more optimized ways to handle constants.

* Calling and return convention

Assume metadata-based precise gc.

Lua explicitly copies return values into place. This makes it possible for a
function to return things not in reg 0..n without explicitly moving its results
into place, probably a good thing. Might be in want of a fast path, though. (In
how many cases can we manage to get return values in registers 0..n w/o
copying?)

Lua also does tailcalls by setting up a frame as usual and then moving the frame
down. Again, allows tailcalling something without overwriting your own args /
explicitly moving args into place. Might turn out to be possible to avoid having
to do this via clever compilation, though. (Could we just fast-path tailcalls
whose args start at 0? Or memmove might already fast-path if src=dst.)

Maybe just expose a "copy register range" instruction? Probably not: it's slower
(more bytecode instructions for a common operation). Might be useful anyways,
but only add if actually needed.

- mmove a b n
  copies b..b+n to a..a+n. expects a < b.

* Labels, jumps and calls

Intra-function jumps are relative (pc offsets). Extra-function jumps/calls are
all indirect (through function pointers or "cells").

* Cells

TODO: Explain cells.

* Precise GC support

Tag bits for now.
_Try_ to make the C API generic enough to work with any.

Options:
1. tag bits. sml does this.

   pros:
   + pretty efficient, space- & time-wise

   cons:
   + not strictly portable (but damn close).
   + fucking the semantics of your language for its implementation.
     ie. "ugh 31 bit ints."

2. gc metadata: structure tags(&maps?), register&stack maps. makes function call
   interface "interesting".

   pros: the best.

   cons:
   + hardest to implement.
   + nigh-impossible to use from a dynamically-typed source language.
     consider (\x. f x)

3. large values (ugh large values). lua does this.

   pros:
   + can make doubles immediate too.

   cons:
   + memory inflation.
   + slow?

4. no immediate numbers. python does this.

   pros:
   + balls simple.

   cons:
   + slooooow (esp if you don't cache small ints)

* Instruction encoding notes

** Comparisons

This section is irrelevant for now, since we're not actually including an
integer comparison instruction yet.

Encoding comparisons is an interesting design point.

We take two operands, and each one could be register, upval, or immediate,
_except_ that we can rule out immediate/immediate comparison. This makes

    8 = 3*3 - 1

possibilities. However, encoding this in the minimum possible 3 bits is a PITA;
the natural encoding uses 4, with 2 bits each to specify the type (reg, upval,
imm) of each operand.

We can make do with only two comparison operations (eg. LEQ, EQ) if we're
willing to be constrained as to which branch goes where. Otherwise we want four
(LT, GEQ, EQ, NEQ). Taking the conditional is cheaper than not taking it, since
we just skip over next instruction without reading it. So not constraining
enables better optimization/performance-tweaking.

The best-performance option is probably an opcode for each combination of
comparison operation and operand types. At minimum there are 8 * 2 = 16
combinations, and at maximum there are 9 * 4 = 36. Writing the code for each
case manually would be insane, but some code-generation scheme could probably be
worked out.

For now, however, we take the simplest option: there is *one* comparison
instruction. It takes the two operands, along with a byte indicating (a) what
types the operands have (reg, upval, or imm) and (b) which comparison is desired
and. (a) is encoded in 4 bits (with the immediate/immediate case representable
but outlawed; this prohibition may or may not be enforced by the bytecode
interpreter) and (b) in 2 bits, so the whole thing can fit in a byte.

If we want our comparison ops to also support floating-point operands with IEEE
semantics, the story gets even more complicated. I'm not worrying about that for
now.

* Language vs. library vs. runtime

Unfortunately the internals of the VM are too tangled up with eris' semantics to
develop it as a separate library. However, eris itself should present a library
interface, a la Lua: it should be embeddable in other C apps.

However, since the plan is to write the compiler in Eris itself & bootstrap,
this means that we can't expose "compile source" functions from liberis itself,
since they're written in Eris! Instead, we expose "load this compiled code"
functionality, and a client app will need to load the byte-compiled code for the
compiler, then invoke the eris compiler through the eris interface. This is kind
of a pain in the ass, but I don't see a better way.

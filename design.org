* Goals

- Pretty efficient to interpret directly.
  - no SSA.
- Register machine, not stack machine
  - some research suggests that reg machines are more performant
  - but mostly because I feel like it.
- Support precise gc.
- Support closures.
  - Non-naive view of environment capture.
    (don't prevent a var from being gced due to a closure that never uses it)

* General notes

Rips off Lua pretty heavily. Register machine, each function indicates how many
registers it needs, registers implicitly on a stack, calling function shifts
register window

* Tags
** Conclusion in brief
There are a lot of decisions to be made about representation of values and their
type tags. Current plan is to put the decision off and just represent values as
pointers to memory blocks with a type header. Go back and optimize later once
the system is built and working.

** Discussion in full
Original scheme: (WORDBITS-1)-bit ints have low bit high. Everything else is a
pointer to an allocated block with a header tag.

The number of bits I use determines the minimum alignment of allocated objects.
2 bits = 4 bytes, which is my alignment constraint already on 32-bit systems. 3
bits = 8 bytes, which you might think would add overhead on a 32-bit system.
However, we're going to need type headers anyway (see below), so all objects
will use at least 4 bytes for a header, plus the content of the object (probably
>= 4 bytes); so it's not really an issue. 4 bits = 16 bytes is probably too
much. A variable-length encoding might be possible, but ugh what a PITA. So we
probably want 3-bit tags.

Let k be the number of tag bits we eventually decide to use. We'll want more
than 2^k types, so we use 2^k-1 tags for common types and reserve one tag for
uncommon types; we determine the actual type of such an object from its header,
which is a pointer to type metadata.

Benefits of this scheme:
1. Faster type testing. It avoids a memory access, but I'm not sure /how/
   important that is. In most cases where we check something's type, we're about
   to be accessing its memory anyway, so further accesses should be cached.

   OTOH, type-testing is /really/ common. At a wild guess I'd say about once per
   two instructions.

   Exceptions to "we're about to be accessing its memory anyway":
   - nil check. but this is just a comparison anyway, since nil is static!

   - symbol equality, which is most common/need-to-be-fast op on symbols. but
     this is fast anyway as long as I only care about results on symbols, since
     equal symbols are identical objects!

   - values that can be stored entirely in a word (incl. tag):
     - fixnums (small ints)
     - builtins

2. (DISCREDITED) Lower memory overhead (eg. conses go from 3-word to 2-word)?

   However, our GC needs to store metadata bits. A type header is the perfect
   place; even if it's a pointer, we can use the lower 2 bits to store the GC
   metadata (simple copying and mark/sweep collectors need only 1 bit).
   Moreover, a mark/sweep collector _needs_ type (or at least size) headers in
   order to scan the heap during the "sweep" phase.

3. (DISCREDITED) GC speedup, since GC can switch on 3-bit tag and special-case
   the common datatypes. But is this really the time-consuming part of GC?
   Probably not.

Disbenefits of it:
1. Slight extra overhead on dealing with "uncommon" types.

   This is especially important wrt closures: if closures are not in the common
   tags list, then calls to closures (super common case) have extra overhead
   (need to check tag bits, then check header).

2. It makes the "undefined" state of cells more interesting, and possibly less
   efficient, to encode.

*** What tags would we use if we had 8 tags (3 bits)?
1. Fixnum
2. Builtin
3. Closure
4. Cons
5. ? Nil
6. ? String
7. ? Symbol
8. Tagged object (? or nil if null)

Unfortunately this is a decision that would optimally be made with the aid of
hard evidence on the performance impacts, but I have no such evidence.

Candidates:
- *Nil/empty list*

  If nil is just a symbol, this is infeasible. On the other hand, could make
  symbols 3-bit tagged. Then comparing against nil is slightly more interesting.
  Could solve "compare with nil" by pre-interning nil, and having it be a
  statically known value. This seems perhaps the best option, as long as I'm
  okay with nil being a symbol.

  Alternatively, nil could have the 3-bit tag of a type that doesn't contain it,
  but whose content is a pointer (not an immediate), and be a NULL pointer. This
  puts extra overhead on the usage of that type, though. Likely types:
  - tagged object (if we're gonna have extra overhead, put it on the uncommon case)
  - symbol (is nil a symbol?)
  - cons (nil is "empty list"; punnery on use of conses for lists)

- *String*
  String manipulation is fairly common.

- *Symbol*
  How common is symbol manipulation, anyway? We don't do it when looking up
  globals. OTOH, if we use symbols for representing branches of ADTs etc, then
  checking symbol equality will be common.

- *Extnum/Boxed number*
  Not sure this is a good idea. Boxed numbers will need tags indicating their
  representation (large integer, rational, float) anyway, so why not just tagged
  object?

Non-candidates:
- *Cell*

  Loading through cells is so common that LOAD_CELL doesn't check that the thing
  it's loading through is in fact a cell; ensuring that is up to the compiler.
  So this doesn't actually need to be 3-bit tagged!

* Builtins

The original plan was to implement builtins (eg. cons, car, cdr, +, -, *, /) as
C functions. Perhaps as "special" C functions that don't get the normal
stack-based treatment, or perhaps not. But in either case, significant overhead
for calling them. Instead we could have another type, builtins, that the
compiler knows how to handle. Code involving "cons" doesn't get compiled
specially - it's still a call through a cell, so "cons" can be
overridden/redefined. But when we actually run the call instruction, it simply
notes that the "function" value is a builtin, and switches on it.

* Upvals & closures

Each fn has environment consisting of "upvals" (closed-over variables; name
stolen from lua, though ours are slightly different).

Closures are created by a "closure" instruction, which takes destination
register, function, and list of operands to populate environment upvals with.
Operands are:
- our registers
- our upvals

Upvals are not indirected; a closure directly contains the upvals needed. This
means they are *copied* when closed-over, so mutations to the register or upval
they came from will not propagate to them and vice versa. This is in contrast to
lua's upvals.

** Big picture re upval copying semantics

There are three possible source-language semantics for variables & closures:
- immutable variables
- mutable variables with copying semantics
- mutable variables with sharing semantics

RVM makes the first two easy to implement efficiently, but an implementation of
the last needs to do some simple analyses to generate efficient RVM code. To
allow sharing, it needs to allocate heap space (perhaps in the form of ref
cells) for the shared variables. But putting every variable on the heap is bad.
So the compiler should only put variables on the heap if they are both (a)
shared between a parent and any of its transitive child functions and (b)
mutated by one of these functions. This is a pretty simple analysis to do, and a
relatively uncommon case in practice.

In fact, there is a name for this analysis/optimization: it is called
"assignment conversion", and (unsurprisingly) appears to have originated in the
design of optimizing scheme compilers. See "ORBIT: An optimizing compiler for
scheme", David Andrew Krantz, 1988.

* Constants

General mechanism for constants is to put them into the upvals of a closure.
Since /all/ functions are closures (no special-case for toplevel funcs), this
always works. I may decide later to add more optimized ways to handle constants.

* Calling and return convention

Assume metadata-based precise gc.

Lua explicitly copies return values into place. This makes it possible for a
function to return things not in reg 0..n without explicitly moving its results
into place, probably a good thing. Might be in want of a fast path, though. (In
how many cases can we manage to get return values in registers 0..n w/o
copying?)

Lua also does tailcalls by setting up a frame as usual and then moving the frame
down. Again, allows tailcalling something without overwriting your own args /
explicitly moving args into place. Might turn out to be possible to avoid having
to do this via clever compilation, though. (Could we just fast-path tailcalls
whose args start at 0? Or memmove might already fast-path if src=dst.)

Maybe just expose a "copy register range" instruction? Probably not: it's slower
(more bytecode instructions for a common operation). Might be useful anyways,
but only add if actually needed.

- mmove a b n
  copies b..b+n to a..a+n. expects a < b.

* Labels, jumps and calls

Intra-function jumps are relative (pc offsets). Extra-function jumps/calls are
all indirect (through function pointers or "cells").

* Cells

TODO: Explain cells.

* Weak refs and finalizers

We can implement both weak references and finalizers without too much difficulty
as follows.

** Weak references
We add a new type of object, weak references. Weak references are either alive
or dead; living weak refs have pointers to their referents; dead weak refs are
just tombstones.

During GC, when we come across a weak reference while scanning the heap, we do
not recursively scan its referent pointer (since it is a *weak* reference).
Instead we put the weak pointer onto a list of "living weak refs". At the end of
the heap scan, we check each living weak ref and see whether its referent is now
alive or dead, and update its state appropriately.

That's it!

** Finalizers
Finalizers can be implemented on top of weak references, either in Eris itself
or hardcoded into the Eris VM, as follows:

A finalizer has a weak reference and a finalizer function. The function mustn't
refer to the target of the weak ref; this will keep the target alive, keeping
the finalizer alive, causing a memory leak.

If implemented in the Eris VM, we keep finalizers in two lists, dormant or
not-yet-run, according to whether their weak references are alive or dead. At
the end of a GC cycle, we scan the dormant list and transfer finalizers with
dead weak refs to the not-yet-run list. It is the host program's responsibility
to run not-yet-run finalizers in a timely fashion, but we need to inform the
host program of new not-yet-run finalizers post-GC; the exact interface here
needs some consideration.

If implemented in Eris, we only need a list of "dormant" finalizers; post-GC, we
call an Eris function that scans this list and removes and calls the finalizers
associated with dead weak refs. Again, the interface with the host program
deserves some consideration.

* Instruction encoding notes

** Comparisons

This section is irrelevant for now, since we're not actually including an
integer comparison instruction yet.

Encoding comparisons is an interesting design point.

We take two operands, and each one could be register, upval, or immediate,
_except_ that we can rule out immediate/immediate comparison. This makes

    8 = 3*3 - 1

possibilities. However, encoding this in the minimum possible 3 bits is a PITA;
the natural encoding uses 4, with 2 bits each to specify the type (reg, upval,
imm) of each operand.

We can make do with only two comparison operations (eg. LEQ, EQ) if we're
willing to be constrained as to which branch goes where. Otherwise we want four
(LT, GEQ, EQ, NEQ). Taking the conditional is cheaper than not taking it, since
we just skip over next instruction without reading it. So not constraining
enables better optimization/performance-tweaking.

The best-performance option is probably an opcode for each combination of
comparison operation and operand types. At minimum there are 8 * 2 = 16
combinations, and at maximum there are 9 * 4 = 36. Writing the code for each
case manually would be insane, but some code-generation scheme could probably be
worked out.

For now, however, we take the simplest option: there is *one* comparison
instruction. It takes the two operands, along with a byte indicating (a) what
types the operands have (reg, upval, or imm) and (b) which comparison is desired
and. (a) is encoded in 4 bits (with the immediate/immediate case representable
but outlawed; this prohibition may or may not be enforced by the bytecode
interpreter) and (b) in 2 bits, so the whole thing can fit in a byte.

If we want our comparison ops to also support floating-point operands with IEEE
semantics, the story gets even more complicated. I'm not worrying about that for
now.

* Language vs. library vs. runtime

Unfortunately the internals of the VM are too tangled up with eris' semantics to
develop it as a separate library. However, eris itself should present a library
interface, a la Lua: it should be embeddable in other C apps.

However, since the plan is to write the compiler in Eris itself & bootstrap,
this means that we can't expose "compile source" functions from liberis itself,
since they're written in Eris! Instead, we expose "load this compiled code"
functionality, and a client app will need to load the byte-compiled code for the
compiler, then invoke the eris compiler through the eris interface. This is kind
of a pain in the ass, but I don't see a better way.
